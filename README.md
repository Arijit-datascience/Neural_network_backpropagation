# Neural Network Feedforward and Backpropagation

## Part 1 - Backpropagation Steps:

### Introduction:
Neural Network consists of neurons and connections between these neurons called weights and some biases connected to each neuron.
We move forward through the network called the forward pass, we iteratively use a formula to calculate each neuron in the next layer.

The network goes forward until we get an output. In this case we have one input layer, one hidden layer and last the output layer.
Our aim is to optimize the cost function. This is done by adjusting the weights based on errors generated by them through a backward pass.
To update the network, we calculate gradients w.r.t. each weights in every layer.

### Network Architechture:
There are two inputs: i1 and i2
Targets: t1 and t2
hidden layer nodes: h1 and h2
activation functions in hidden layers: a_h1 and a_h2
outputs: o1 and o2
activation functions in output layers: a_o1 and a_o2
errors: E1 and E2
weights: w1, w2, w3, w4, w5, w6, w7 and w8

![feedforward](https://user-images.githubusercontent.com/65554220/119848614-94721180-bf29-11eb-91e7-989e09bb1d98.JPG)

### Feed Forward Calculations:
#### Hidden layer:
h1 is getting input from i1 and i2 connected with w1 and w2 respectively. The hidden layer is calculated as below:

h1 = w1*i1 + w2*i2

Similarly, h2 is calculated as:

h2 = w3*i1 + w4*i2

#### Activation function at hidden layer:
In this example we are using sigmoid as the activation function.

a_h1 = ùúé(h1) = 1/(1 + exp(-h1))

Similarly, a_h2 is calculated as:

a_h2 = ùúé(h2)

#### Output layer:
o1 is getting input from a_h1 and a_h2 connected with w5 and w6 respectively. The output layer is calculated as below:

o1 = w5*a_h1 + w6*a_h2

Similarly, o2 is calculated as:

o2 = w7*a_h1 + w8*a_h2

#### Activation function at output layer:
In this example we are using sigmoid as the activation function.

a_o1 = ùúé(o1) = 1/(1 + exp(-o1))

Similarly, a_o2 is calculated as:

a_o2 = ùúé(o2)

#### Error calculations:
E1 = 1/2 * (t1 - a_o1)^2

E2 = 1/2 * (t2 - a_o2)^2

E_total = E1 + E2


### Weight updation in next iteration
We simply calculate gradient of cost function w.r.t. the weight and multiply with the learning rate. Then the previous weight is subtracted by this learning rate gradient factor.
![image](https://user-images.githubusercontent.com/65554220/119844436-0d6f6a00-bf26-11eb-9fad-986fccb1099e.png)

Below image shows the effect of learning rates on convergence.
As the learning rate is increased the convergence becomes faster. As the learning rate increase, there will be a point where loss stops decreasing and starts increasing.
We should select learning rate such that the losses are minimized and converge at global minima faster.
In this example, we do not have any very high learning rate. Learning rate of 0.1 is too low in this case whereas 2 is a very good learning rate from the curves.  

![learning rate](https://user-images.githubusercontent.com/65554220/119846043-6a1f5480-bf27-11eb-8487-4cf95a00d3d8.JPG)

The below screenshot is from the excel file "Back Propagation.xlsx" in the repository.
This shows how feed forward network and back propagation works.

![neural_network_feedforward_backpropagation](https://user-images.githubusercontent.com/65554220/119373201-cb4fe980-bcd5-11eb-82a4-01ef1e6cc6d0.JPG)

